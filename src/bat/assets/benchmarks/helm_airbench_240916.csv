model,HELM_AIRBENCH_Security_Risks,HELM_AIRBENCH_Operational_Misuses,HELM_AIRBENCH_Violence_&_Extremism,HELM_AIRBENCH_Hate/Toxicity,HELM_AIRBENCH_Sexual_Content,HELM_AIRBENCH_Child_Harm,HELM_AIRBENCH_Self-harm,HELM_AIRBENCH_Political_Usage,HELM_AIRBENCH_Economic_Harm,HELM_AIRBENCH_Deception,HELM_AIRBENCH_Manipulation,HELM_AIRBENCH_Defamation,HELM_AIRBENCH_Fundamental_Rights,HELM_AIRBENCH_Discrimination/Bias,HELM_AIRBENCH_Privacy,HELM_AIRBENCH_Criminal_Activities
Claude 3 Haiku (20240307),0.005,0.572,0.159,0.057,0.219,0.152,0.022,0.041,0.04,0.089,0.053,0.037,0,0.382,0.086,0.008
Claude 3 Sonnet (20240229),0.009,0.473,0.156,0.071,0.184,0.133,0,0.031,0.02,0.096,0.127,0.074,0,0.332,0.089,0
Llama 3 Instruct (8B),0.065,0.694,0.214,0.188,0.208,0.276,0.089,0.177,0.14,0.259,0.353,0.38,0.027,0.521,0.225,0
Claude 3 Opus (20240229),0.065,0.477,0.187,0.091,0.34,0.19,0.022,0.063,0.06,0.126,0.1,0.074,0,0.27,0.096,0.017
Gemini 1.5 Pro (001),0.097,0.338,0.253,0.135,0.288,0.233,0.078,0.161,0.09,0.215,0.22,0.194,0.06,0.24,0.123,0.042
Gemini 1.5 Flash (001),0.124,0.371,0.289,0.164,0.302,0.286,0.022,0.195,0.153,0.3,0.293,0.278,0.053,0.325,0.14,0.033
GPT-3.5 Turbo (0613),0.137,0.551,0.455,0.274,0.549,0.429,0.089,0.463,0.433,0.522,0.433,0.463,0.213,0.516,0.316,0.108
GPT-4 Turbo (2024-04-09),0.142,0.636,0.329,0.144,0.378,0.381,0.156,0.323,0.293,0.304,0.34,0.167,0.08,0.461,0.207,0.058
Llama 3 Instruct (70B),0.158,0.726,0.351,0.329,0.49,0.267,0.078,0.339,0.34,0.385,0.427,0.574,0.147,0.502,0.274,0.025
GPT-3.5 Turbo (1106),0.275,0.636,0.589,0.433,0.559,0.629,0.322,0.609,0.623,0.659,0.573,0.481,0.333,0.589,0.39,0.267
GPT-4o (2024-05-13),0.297,0.813,0.527,0.327,0.524,0.552,0.189,0.601,0.587,0.504,0.54,0.426,0.267,0.575,0.45,0.233
GPT-3.5 Turbo (0125),0.405,0.768,0.664,0.51,0.667,0.752,0.422,0.725,0.71,0.748,0.7,0.593,0.52,0.624,0.471,0.45
Qwen1.5 Chat (72B),0.453,0.772,0.579,0.371,0.635,0.686,0.356,0.616,0.623,0.733,0.633,0.63,0.467,0.571,0.546,0.35
DeepSeek LLM Chat (67B),0.457,0.709,0.541,0.365,0.622,0.643,0.344,0.532,0.567,0.648,0.573,0.407,0.373,0.584,0.515,0.3
Yi Chat (34B),0.509,0.691,0.558,0.377,0.576,0.624,0.289,0.52,0.503,0.681,0.533,0.491,0.227,0.559,0.436,0.275
Mixtral Instruct (8x22B),0.671,0.744,0.726,0.417,0.569,0.767,0.322,0.747,0.647,0.726,0.66,0.463,0.573,0.593,0.593,0.646
Mixtral Instruct (8x7B),0.777,0.818,0.733,0.504,0.632,0.848,0.533,0.808,0.74,0.822,0.687,0.602,0.627,0.592,0.579,0.742
Cohere Command R,0.782,0.878,0.775,0.586,0.712,0.824,0.578,0.861,0.82,0.822,0.813,0.648,0.773,0.678,0.699,0.717
Cohere Command R Plus,0.829,0.881,0.816,0.653,0.729,0.819,0.578,0.895,0.897,0.867,0.853,0.815,0.8,0.68,0.709,0.817
Mistral Instruct v0.3 (7B),0.932,0.841,0.806,0.501,0.597,0.924,0.522,0.909,0.91,0.889,0.853,0.648,0.893,0.624,0.717,0.942
DBRX Instruct,0.955,0.874,0.841,0.624,0.684,0.924,0.722,0.963,0.953,0.926,0.953,0.75,0.947,0.675,0.817,0.967